{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOCUMENTO PARA AUTOMATIZAR EL PROCESO DE ETL , DATOS DE UBER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerias a importar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTADO PARA WEBSCRAPPING\n",
    "\n",
    "import re #Expresiones regulraes\n",
    "from colorama import Fore\n",
    "import requests\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAR PARA ETL Y REDUCCION DE DATOS\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CARGA DE DATOS PARA LEER -- DESPUES BORRAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"Data/uber_final.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEBSCRAPING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colocar el WEBCRAPIONG de leando en vez de este\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descarga todos los archivos de la pagina web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La web la que queremos scrappear\n",
    "website_uber = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "#Hacemos una peticion al sitio web que seleccionamos\n",
    "resultado_uber = requests.get(website_uber)\n",
    "#Convertimos el resultado a texto\n",
    "content_uber = resultado_uber.text\n",
    "#Para hacer webscrapping tengo que encontrar en el codigo de html de la pagina un patron en comun que tengan\n",
    "#las cosas que quiero extraer [\\w-]*.* detecta todo lo que va despues de lo que asignamos en patron\n",
    "patron_uber = r'<a\\s+href=\"(.*?/trip-data/fhvhv_tripdata[^\"]*)\"'\n",
    "#Queremos buscar con la expresion regular del patron dentro de content\n",
    "maquinas_uber = re.findall(patron_uber, str(content_uber))\n",
    "#En caso de haber duplicados debemos hacer lo siguiente:\n",
    "#sin_duplicados = list(set(maquinas_repetidas))\n",
    "# Debemos dejar solamente el nombre ejemplo tenemos 'data-answer=\"faq2023\" class=\"faq-questions\" y debe estar solo faq2023\n",
    "\n",
    "\n",
    "maquinas_final_uber = []\n",
    "nombre_archivos_uber = []\n",
    "for i in maquinas_uber:\n",
    "    if int(i[63:67]) > 2018 and int(i[63:67]) <= 2023:\n",
    "        maquinas_final_uber.append(i)\n",
    "        nombre_archivos_uber.append(i[48:79])\n",
    "        print(i)\n",
    "\n",
    "nombre_archivos_uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descarga todo en el almacenamiento local\n",
    "\n",
    "# for a, b in zip(maquinas_final_uber, nombre_archivos_uber):\n",
    "    urllib.request.urlretrieve(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIN WEBSCRAPING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducción de datos al 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducir_archivo_uber(carpeta_origen, carpeta_destino):\n",
    "    # Carpeta_origen, es la ruta de dicha carpeta\n",
    "    # Carpeta_destino, es la ruta de la carpeta a ser guardado\n",
    "\n",
    "    # Obtener la lista de archivos Parquet en la carpeta de origen\n",
    "    archivos_parquet = [archivo for archivo in os.listdir(carpeta_origen) if archivo.endswith('.parquet')]\n",
    "\n",
    "    # Iterar sobre cada archivo Parquet\n",
    "    for archivo in archivos_parquet:\n",
    "        ruta_parquet = os.path.join(carpeta_origen, archivo)\n",
    "\n",
    "        # Leer el archivo Parquet y convertirlo a DataFrame\n",
    "        tabla_parquet = pq.read_table(ruta_parquet)\n",
    "        dataframe = tabla_parquet.to_pandas()\n",
    "\n",
    "        # Realizar las operaciones necesarias en el DataFrame\n",
    "        dataframe = dataframe.sample(frac=0.05, random_state=123).reset_index(drop=True)\n",
    "\n",
    "        # Crear la ruta para el archivo de salida en la carpeta destino\n",
    "        ruta_parquet_destino = os.path.join(carpeta_destino, archivo)\n",
    "\n",
    "        # Guardar el DataFrame\n",
    "        dataframe.to_parquet(ruta_parquet_destino, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARA VER!! --->  Ver tema carpetas en Google cloud!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "carpeta_origen = 'Data/uber'\n",
    "carpeta_destino = 'Data/uber/Reducido' \n",
    "reducir_archivo_uber(carpeta_origen, carpeta_destino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL DEL DATA SET DE UBERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VER BIEN DE DONDE LO VAMOS A TOMAR! <BR>\n",
    "SI LO CARGAMOS DESDE UN DRIVE, HAY QUE PONER ESE PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se carga para lectura el dataset brindado por TLC que posee el ID y sus respectivas localidades, distritos, zonas y zonas de servicio.\n",
    "\n",
    "path = 'Data/taxi+_zone_lookup.csv'\n",
    "\n",
    "taxis_zones = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se rellenan los nulos con el valor 'Unknown' \n",
    "taxis_zones.fillna('Unknown', inplace=True)\n",
    "\n",
    "# Se dropea la columna LocationID\n",
    "taxis_zones = taxis_zones.drop('service_zone', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxis_zones.to_parquet(\"Data/taxis_zones.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigue con el dataset de UBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se entiende que los registros nulos se deben a que son viajes que no fueron tomados en zona de aeropuertos, se procede a rellenarlos con valor tasa '0.00'\n",
    "uber.fillna(0.00, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea una lista con las columnas a sumar\n",
    "columnas_a_sumar = ['base_passenger_fare', 'tolls', 'bcf','sales_tax','congestion_surcharge','airport_fee','driver_pay']\n",
    "\n",
    "# Se crea una nueva columna 'total_fare' que contiene la suma de las columnas de costos\n",
    "uber['total_fare'] = uber[columnas_a_sumar].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea una lista con las columnas a borrar\n",
    "columnasAborrar = ['base_passenger_fare', 'tolls', 'bcf','sales_tax','congestion_surcharge','airport_fee','driver_pay']\n",
    "\n",
    "# Se dropean las columnas enlistadas\n",
    "uber_sumado = uber.drop(columnasAborrar, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar la fusión (merge) basándote en las columnas LocationID, en este caso utilizando la localización de recogida\n",
    "uber_merge = pd.merge(uber_sumado, taxis_zones, left_on='PULocationID', right_on='LocationID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombre de columnas\n",
    "uber_merge.rename(columns={'Borough': 'PUBorough','Zone' : 'PUZone' }, inplace=True)\n",
    "\n",
    "# Se dropea la columna LocationID\n",
    "uber_merge = uber_merge.drop('LocationID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_merge_parquet = uber_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se realiza el merge usando las columnas de LocationID, en este caso utilizando la localización de arribo\n",
    "uber_merge2 = pd.merge(uber_merge_parquet, taxis_zones, left_on='DOLocationID', right_on='LocationID', how='inner')\n",
    "\n",
    "# Se renombran las columnas necesarias\n",
    "uber_merge2.rename(columns={'Borough': 'DOBorough','Zone' : 'DOZone' }, inplace=True)\n",
    "\n",
    "# Se dropea la columna LocationID\n",
    "uber_merge2 = uber_merge2.drop('LocationID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se elimina la columna que contiene la hora en que llego el conductor al lugar de recogida.\n",
    "# Podria eliminarse tambien la columna de request_datetime, ya que a los objetivos del proyecto no estaría aportando algo valioso.\n",
    "columnasAborrar = ['on_scene_datetime', 'request_datetime']\n",
    "\n",
    "uber_zone = uber_merge2.drop(columnasAborrar, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de nombres de columnas que deseas convertir a tipo datetime\n",
    "columnas_a_convertir = ['pickup_datetime','dropoff_datetime']\n",
    "\n",
    "# Itera sobre cada columna y realiza la conversión a tipo datetime\n",
    "for columna in columnas_a_convertir:\n",
    "    uber_zone[columna] = pd.to_datetime(uber_zone[columna], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orden = ['pickup_datetime','PULocationID','PUBorough','PUZone','dropoff_datetime','DOLocationID','DOBorough','DOZone','trip_miles','trip_time','tips','total_fare']\n",
    "\n",
    "# Reordena las columnas según el nuevo orden\n",
    "uber_zone = uber_zone[orden]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se renombran las columnas necesarias\n",
    "uber_zone.rename(columns={'pickup_datetime': 'PUDatetime','dropoff_datetime' : 'DODatetime' }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber = uber_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea dos nuevas columnas: 'fecha' y 'hora' -- para Pick Up y Drop Off\n",
    "uber['PUDate'] = uber['PUDatetime'].dt.date\n",
    "uber['PUTime'] = uber['PUDatetime'].dt.time\n",
    "\n",
    "uber['DODate'] = uber['DODatetime'].dt.date\n",
    "uber['DOTime'] = uber['DODatetime'].dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnasAborrar = ['PUDatetime', 'DODatetime']\n",
    "\n",
    "uber = uber.drop(columnasAborrar, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orden = ['PUDate','PUTime','PULocationID','PUBorough','PUZone','DODate','DOTime','DOLocationID','DOBorough','DOZone','trip_miles','trip_time','tips','total_fare']\n",
    "\n",
    "# Reordena las columnas según el nuevo orden\n",
    "uber = uber[orden]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIN DE ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CARGA DE DATOS EN GOOGLE CLOUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIN DE CARGA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "----\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARA HACER AL FINAL EN LA TERMINAL Y GENERAR EL ARCHIVO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip freeze | awk -F'==' '{print $1}' > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
